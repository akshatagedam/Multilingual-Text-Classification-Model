# -*- coding: utf-8 -*-
"""Multilingual Text Classification Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Xa9MSP5CzHvM7H_VH-SgmovNBF0Pb8In
"""

!pip install emoji

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import string
import re
import codecs
import emoji
from sklearn.model_selection import train_test_split
from sklearn import linear_model
from sklearn import feature_extraction, pipeline, metrics
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

from gensim.models import Word2Vec
from nltk.tokenize import word_tokenize
import random


import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

!pip install gensim

data_file = '/content/all_annotated.tsv'
data = pd.read_csv(data_file,sep='\t')

data

distinct_countries = data['Country'].unique()
distinct_countries

print("number of countries is:", len(distinct_countries))

"""**Data Preprocessing**"""

data = data[['Country','Tweet']]

data

for char in string.punctuation:
    print(char, end=" ")
translate_table = dict((ord(char), None) for char in string.punctuation)

df = pd.DataFrame(data)

print(df)

eng_df = df[df['Country'] == 'US']
eng_df.rename(columns={'Country':'English'}, inplace=True)
print(eng_df)

span_df = df[df['Country'] == 'AR']
span_df.rename(columns={'Country':'Spanish'}, inplace=True)
print(span_df)

turc_df = df[df['Country'] == 'TR']
turc_df.rename(columns={'Country':'Turkish'}, inplace=True)
print(turc_df)

dz_df = df[df['Country'] == 'DZ']
dz_df.rename(columns={'Country':'Darija'}, inplace=True)
print(dz_df)

italian_df = df[df['Country'] == 'IT']
italian_df.rename(columns={'Country':'Italian'}, inplace=True)
print(italian_df)

"""**Cleaning Text for all the languages**



"""

data_eng = []
lang_eng = []
data_it = []
lang_it = []


def clean_text(text):
    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = text.translate(translate_table)
    text = emoji.replace_emoji(text, replace='')
    return text

"""**Cleaning Text for English**"""

for i, row in eng_df.iterrows():
    line = row['Tweet']
    if len(line) != 0:
        line = clean_text(line)
        data_eng.append(line)
        lang_eng.append("English")

df_english = pd.DataFrame({'Cleaned_Tweet': data_eng, 'Language': lang_eng})
df_english

"""**Cleaning Text for Italian**"""

for i, row in italian_df.iterrows():
    line = row['Tweet']
    if len(line) != 0:
        line = clean_text(line)
        data_it.append(line)
        lang_it.append("Italian")

italian_df = pd.DataFrame({'Cleaned_Tweet': data_it, 'Language': lang_it})
italian_df

"""**Data augmentation for Italian**

"""

text_column = 'Cleaned_Tweet'
texts = italian_df[text_column].tolist()

import nltk
nltk.download('punkt')

tokenized_texts = [word_tokenize(text) for text in texts]

#Trainig Word2Vec model on italian texts
model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4)

def augment_text(text, model, num_augmented_sentences=5):
    words = word_tokenize(text)
    augmented_texts = []

    for _ in range(num_augmented_sentences):
        new_words = []
        for word in words:
            if word in model.wv.key_to_index and random.random() > 0.7:  # 30% chance to replace
                similar_words = model.wv.most_similar(word, topn=5)
                new_word = random.choice(similar_words)[0]
                new_words.append(new_word)
            else:
                new_words.append(word)
        augmented_texts.append(' '.join(new_words))

    return augmented_texts

# Apply the augmentation to the text column
italian_df['Augmented_Tweet'] = italian_df[text_column].apply(lambda x: augment_text(x, model))

# Verify the results
print(italian_df.head())

all_texts = italian_df[text_column].tolist() + [aug_text for sublist in italian_df['Augmented_Tweet'].tolist() for aug_text in sublist]
all_labels = italian_df['Language'].tolist() * (1 + len(italian_df['Augmented_Tweet'].iloc[0]))

italian_df_augmented = pd.DataFrame({
    'Cleaned_Tweet': all_texts,
    'Language': all_labels
})

print(italian_df_augmented)
print(italian_df_augmented.shape)

"""**Cleaning Text for Turkish**"""

data_turk = []
lang_turk = []

for i, row in turc_df.iterrows():
    line = row['Tweet']
    if len(line) != 0:
        line = clean_text(line)
        data_turk.append(line)
        lang_turk.append("Turkish")

turc_df = pd.DataFrame({'Cleaned_Tweet': data_turk, 'Language': lang_turk})
turc_df

"""**Cleaning Text for Spanish**"""

data_spanish = []
lang_spanish = []

for i, row in span_df.iterrows():
    line = row['Tweet']
    if len(line) != 0:
        line = clean_text(line)
        data_spanish.append(line)
        lang_spanish.append("Spanish")

span_df = pd.DataFrame({'Cleaned_Tweet': data_spanish, 'Language': lang_spanish})
span_df

"""**Data augmentation on Spanish**"""

texts = span_df[text_column].tolist()
tokenized_texts = [word_tokenize(text) for text in texts]

# Apply the augmentation to the text column
span_df['Augmented_Tweet'] = span_df[text_column].apply(lambda x: augment_text(x, model))

# Verify the results
print(span_df.head())

all_texts = span_df[text_column].tolist() + [aug_text for sublist in span_df['Augmented_Tweet'].tolist() for aug_text in sublist]
all_labels = span_df['Language'].tolist() * (1 + len(span_df['Augmented_Tweet'].iloc[0]))

spanish_df_augmented = pd.DataFrame({
    'Cleaned_Tweet': all_texts,
    'Language': all_labels
})

print(spanish_df_augmented)
print(spanish_df_augmented.shape)

"""**Cleaned Df**"""

cleaned_df = pd.concat([df_english,italian_df_augmented,spanish_df_augmented,turc_df], ignore_index=True)
print(cleaned_df.shape)

cleaned_df

# Ensure all entries in 'Cleaned_Tweet' are strings
non_string_entries = cleaned_df[~cleaned_df['Cleaned_Tweet'].apply(lambda x: isinstance(x, str))]

if not non_string_entries.empty:
    print("Non-string entries found in 'Cleaned_Tweet':")
    print(non_string_entries)
else:
    print("All entries in 'Cleaned_Tweet' are valid strings.")

X, y = cleaned_df.iloc[:,0], cleaned_df.iloc[:,1]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

print(X_train.shape)
print(X_test.shape)
print(y_train.shape)
print(y_test.shape)

print(X_train)

print(y)

"""**Vectorisation**"""

vectorizer = feature_extraction.text.TfidfVectorizer(ngram_range=(1,3), analyzer='char')

pipe_lr_r13 = pipeline.Pipeline([
    ('vectorizer', vectorizer),
    ('clf', linear_model.LogisticRegression())
])

"""**Model Fitting**"""

pipe_lr_r13.fit(X_train.values.astype('U'), y_train)

y_predicted = pipe_lr_r13.predict(X_test.values.astype('U'))

acc = (metrics.accuracy_score(y_test, y_predicted))*100
print(acc, '%')

labels = np.unique(np.concatenate((y_test, y_predicted)))
labels

matrix = metrics.confusion_matrix(y_test, y_predicted, labels=labels)
print('Confusion metrix: \n', matrix)

plt.figure(figsize=(10, 8))
sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues',xticklabels=labels, yticklabels=labels)
plt.title('Confusion Matrix')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
# Rotate x-axis labels
plt.xticks(rotation=45, ha='right')

# Adjust layout to prevent cutting off labels
plt.tight_layout()
plt.show()

import pickle

lrFile = open('LRModel.pckl', 'wb')
pickle.dump(pipe_lr_r13, lrFile)
lrFile.close()

global lrLangDetectModel
lrLangDetectFile = open('LRModel.pckl', 'rb')
lrLangDetectModel = pickle.load(lrLangDetectFile)
lrLangDetectFile.close()

def tweet_lang_detect(text):
    translate_table = dict((ord(char), None) for char in string.punctuation)
    global lrLangDetectModel
    lrLangDetectFile = open('LRModel.pckl', 'rb')
    lrLangDetectModel = pickle.load(lrLangDetectFile)
    lrLangDetectFile.close()

    text = text.lower()
    text = re.sub(r'\d+', '', text)
    text = emoji.replace_emoji(text, replace='')
    text = text.translate(translate_table)

    pred = lrLangDetectModel.predict([text])
    prob = lrLangDetectModel.predict_proba([text])
    return pred[0]

tweet_lang_detect("hello, what a beautiful day ! ")

tweet_lang_detect("Merhaba, ne güzel bir gün!")

tweet_lang_detect("hola, ¡qué hermoso día!")

tweet_lang_detect("ciao, che bella giornata!")

